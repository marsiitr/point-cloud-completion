{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T04:28:58.977554Z",
     "iopub.status.busy": "2024-06-21T04:28:58.977192Z",
     "iopub.status.idle": "2024-06-21T04:29:13.186468Z",
     "shell.execute_reply": "2024-06-21T04:29:13.185664Z",
     "shell.execute_reply.started": "2024-06-21T04:28:58.977523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 04:29:01.304109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-21 04:29:01.304202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-21 04:29:01.478767: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T04:29:16.466913Z",
     "iopub.status.busy": "2024-06-21T04:29:16.465874Z",
     "iopub.status.idle": "2024-06-21T04:34:43.035673Z",
     "shell.execute_reply": "2024-06-21T04:34:43.034856Z",
     "shell.execute_reply.started": "2024-06-21T04:29:16.466878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://download.cs.stanford.edu/downloads/completion3d/dataset2019.zip\n",
      "\u001b[1m1585860897/1585860897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = tf.keras.utils.get_file(\n",
    "    \"dataset2019.zip\",\n",
    "    \"http://download.cs.stanford.edu/downloads/completion3d/dataset2019.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "DATA_DIR = os.path.join (os.path.dirname(DATA_DIR),\"shapenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T04:34:51.538206Z",
     "iopub.status.busy": "2024-06-21T04:34:51.537638Z",
     "iopub.status.idle": "2024-06-21T04:34:51.543451Z",
     "shell.execute_reply": "2024-06-21T04:34:51.542482Z",
     "shell.execute_reply.started": "2024-06-21T04:34:51.538175Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def read_point_cloud_from_h5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        \n",
    "        point_cloud_data = file['data'][:]\n",
    "    return point_cloud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T05:20:26.419374Z",
     "iopub.status.busy": "2024-06-21T05:20:26.418692Z",
     "iopub.status.idle": "2024-06-21T05:20:54.621883Z",
     "shell.execute_reply": "2024-06-21T05:20:54.620914Z",
     "shell.execute_reply.started": "2024-06-21T05:20:26.419344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 02691156: 100%|██████████| 3795/3795 [00:03<00:00, 1088.31it/s]\n",
      "Loading 03636649: 100%|██████████| 2068/2068 [00:01<00:00, 1095.93it/s]\n",
      "Loading 02933112: 100%|██████████| 1322/1322 [00:01<00:00, 1084.20it/s]\n",
      "Loading 02958343: 100%|██████████| 5677/5677 [00:05<00:00, 1079.16it/s]\n",
      "Loading 03001627: 100%|██████████| 5750/5750 [00:05<00:00, 1068.83it/s]\n",
      "Loading 04256520: 100%|██████████| 2923/2923 [00:02<00:00, 1080.93it/s]\n",
      "Loading 04379243: 100%|██████████| 5750/5750 [00:05<00:00, 1037.04it/s]\n",
      "Loading 04530566: 100%|██████████| 1689/1689 [00:01<00:00, 983.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28974 partial point clouds and 28974 ground truth point clouds.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_h5_file(file_path, dataset_name='data'):\n",
    "    \"\"\"Load point cloud data from an .h5 file.\"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        data = file[dataset_name][:]\n",
    "    return data\n",
    "\n",
    "def load_dataset(base_dir, categories, dataset_name='data'):\n",
    "    \"\"\"Load partial and gt datasets from the given base directory.\"\"\"\n",
    "    partials = []\n",
    "    gts = []\n",
    "\n",
    "    for category in categories:\n",
    "        partial_dir = os.path.join(base_dir, 'partial', category)\n",
    "        gt_dir = os.path.join(base_dir, 'gt', category)\n",
    "\n",
    "        # Check if the directories exist\n",
    "        if not os.path.exists(partial_dir):\n",
    "            print(f\"Partial directory does not exist: {partial_dir}\")\n",
    "            continue\n",
    "        if not os.path.exists(gt_dir):\n",
    "            print(f\"GT directory does not exist: {gt_dir}\")\n",
    "            continue\n",
    "\n",
    "        partial_files = sorted([f for f in os.listdir(partial_dir) if f.endswith('.h5')])\n",
    "        gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith('.h5')])\n",
    "\n",
    "        for p_file, gt_file in tqdm(zip(partial_files, gt_files), total=len(partial_files), desc=f\"Loading {category}\"):\n",
    "            partial_path = os.path.join(partial_dir, p_file)\n",
    "            gt_path = os.path.join(gt_dir, gt_file)\n",
    "\n",
    "            partial_data = load_h5_file(partial_path, dataset_name)\n",
    "            gt_data = load_h5_file(gt_path, dataset_name)\n",
    "\n",
    "            partials.append(partial_data)\n",
    "            gts.append(gt_data)\n",
    "\n",
    "    return np.array(partials), np.array(gts)\n",
    "\n",
    "# Example usage\n",
    "base_dir = os.path.join(DATA_DIR , \"train\")\n",
    "categories = ['02691156', '03636649', '02933112', '02958343', '03001627' , '04256520' , '04379243' , '04530566']  # Replace with your actual category names\n",
    "partial_dataset, gt_dataset = load_dataset(base_dir, categories)\n",
    "print(f'Loaded {len(partial_dataset)} partial point clouds and {len(gt_dataset)} ground truth point clouds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
